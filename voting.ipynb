{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imblearn,optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix,f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "import optuna as op\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install dill\n",
    "import dill\n",
    "dill.dump_session('env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31647, 32)\n",
      "0    27909\n",
      "1    27909\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"bank_seasons.csv\")\n",
    "df['y'].value_counts()\n",
    "#imbalanced dataset\n",
    "\n",
    "#split dataset\n",
    "x=df.drop('y', axis=1)\n",
    "y=df['y']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=1,train_size=0.7)\n",
    "print(x_train.shape)\n",
    "#print(x_test.shape)\n",
    "\n",
    "#balance dataset (SMOTE)\n",
    "smote=SMOTE(random_state=1) \n",
    "x_train, y_train = smote.fit_resample(x_train,y_train)\n",
    "print(y_train.value_counts())\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train = pd.DataFrame(ss.fit_transform(x_train),columns=x_train.columns)\n",
    "x_test = pd.DataFrame(ss.transform(x_test),columns=x_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install xgboost\n",
    "# %pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.9657995628650257\n",
      "testing acc  0.8962695370097317\n",
      "testing f1  0.5388397246804325\n",
      "[11335   678   729   822]\n",
      "0.5299806576402321\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(),xgb)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.9227989840088721\n",
      "testing acc  0.8751105868475376\n",
      "testing f1  0.47748303516347934\n",
      "[11096   873   821   774]\n",
      "0.4852664576802508\n"
     ]
    }
   ],
   "source": [
    "adab = AdaBoostClassifier()\n",
    "clf = make_pipeline(StandardScaler(),adab)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.neighbors  import   KNeighborsClassifier \n",
    "from   sklearn.ensemble  import   BaggingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.9482425024185747\n",
      "testing acc  0.8852108522559717\n",
      "testing f1  0.41750841750841744\n",
      "[11449   564   993   558]\n",
      "0.3597678916827853\n"
     ]
    }
   ],
   "source": [
    "bagg = BaggingClassifier(KNeighborsClassifier(), max_samples= 0.5, max_features=0.5)\n",
    "clf = make_pipeline(StandardScaler(),bagg)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.naive_bayes  import   GaussianNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.8283349457164355\n",
      "testing acc  0.7402683574166913\n",
      "testing f1  0.2869864399919045\n",
      "[9332 2681  842  709]\n",
      "0.4571244358478401\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "clf = make_pipeline(StandardScaler(),gnb)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.discriminant_analysis  import   QuadraticDiscriminantAnalysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.8843921315704611\n",
      "testing acc  0.81664700678266\n",
      "testing f1  0.32729239924262915\n",
      "[10472  1541   946   605]\n",
      "0.3900709219858156\n"
     ]
    }
   ],
   "source": [
    "da = QuadraticDiscriminantAnalysis()\n",
    "clf = make_pipeline(StandardScaler(),da)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.ensemble  import   GradientBoostingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.9231789028628757\n",
      "testing acc = 0.8815246240047184\n",
      "testing f1 = 0.5195814648729448\n",
      "confusion matrix = [11088   925   682   869]\n",
      "recall = 0.5602836879432624\n"
     ]
    }
   ],
   "source": [
    "gbdt = GradientBoostingClassifier()\n",
    "clf = make_pipeline(StandardScaler(),gbdt)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc =\",scores1)\n",
    "print(\"testing f1 =\",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(\"c m =\",cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(\"recall =\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier \n",
    "import xgboost \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.955050342183525\n",
      "testing acc = 0.8970067826599823\n",
      "testing f1 = 0.5459863503412415\n",
      "confusion matrix = [11327   686   711   840]\n",
      "recall = 0.5415860735009671\n"
     ]
    }
   ],
   "source": [
    "clf1 = GradientBoostingClassifier(n_estimators = 200 ) \n",
    "clf2 = RandomForestClassifier(random_state = 0 , n_estimators = 500 ) \n",
    "clf3 = LogisticRegression(random_state=1) \n",
    "clf4 = GaussianNB() \n",
    "clf5 = xgboost.XGBClassifier() \n",
    "clf6 = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=5)\n",
    "clf  = VotingClassifier(estimators = [ ('gbdt',clf1), \n",
    "        #   ( 'rf' ,clf2), \n",
    "         # ('lr',clf3), \n",
    "         #('nb',clf4), \n",
    "          ('xgboost',clf5),\n",
    "        #   ('SGDClassifier',clf6) \n",
    "      ], \n",
    "          voting = 'soft' \n",
    "      )  \n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc =\",scores1)\n",
    "print(\"testing f1 =\",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(\"confusion matrix =\",cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(\"recall =\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM #SVM change parameters kernel='rbf'  the best\n",
    "clf = make_pipeline(StandardScaler(),SVC(gamma='auto'))\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM change parameters kernel='linear'\n",
    "clf = make_pipeline(StandardScaler(),SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, \n",
    "                                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None))\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM change parameters kernel='poly'\n",
    "clf = make_pipeline(StandardScaler(),SVC(C=1.0, kernel='poly', degree=3, gamma='scale', coef0=0.0, shrinking=True, \n",
    "                                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None))\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM change parameters kernel='sigmoid'\n",
    "clf = make_pipeline(StandardScaler(),SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale', coef0=0.0, shrinking=True, \n",
    "                                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None))\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM change parameters kernel='sigmoid'\n",
    "clf = make_pipeline(StandardScaler(),SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale', coef0=0.0, shrinking=True, \n",
    "                                         probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                                         max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None))\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "clf = make_pipeline(StandardScaler(),tree.DecisionTreeClassifier())\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDClassifier\n",
    "\n",
    "        - 'hinge' gives a linear SVM.\n",
    "        - 'log_loss' gives logistic regression, a probabilistic classifier.\n",
    "        - 'modified_huber' is another smooth loss that brings tolerance to\n",
    "           outliers as well as probability estimates.\n",
    "        - 'squared_hinge' is like hinge but is quadratically penalized.\n",
    "        - 'perceptron' is the linear loss used by the perceptron algorithm.\n",
    "        - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n",
    "          'squared_epsilon_insensitive' are designed for regression but can be useful\n",
    "          in classification as well; see\n",
    "          :class:`~sklearn.linear_model.SGDRegressor` for a description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf =  SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf =  SGDClassifier(loss=\"huber\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc  0.8946755526890967\n",
      "testing acc  0.8319079917428487\n",
      "testing f1  0.3893947509373326\n",
      "[10557  1456   824   727]\n",
      "0.4687298517085751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/ExternalDisk/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf =  SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KN neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf =   KNeighborsClassifier(n_neighbors=100)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = clf.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n",
    "\n",
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_mlp = MLPClassifier(max_iter=99999999,alpha = 0.01,hidden_layer_sizes=(128,65,35,16,8),learning_rate= 'adaptive',solver='adam',batch_size=64)\n",
    "model_mlp.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model_mlp.predict(x_train)\n",
    "\n",
    "scores = metrics.accuracy_score(y_train,y_pred)\n",
    "print(\"training acc \",scores)\n",
    "\n",
    "y_pred1 = model_mlp.predict(x_test)\n",
    "scores1 = metrics.accuracy_score(y_test,y_pred1)\n",
    "f11 = metrics.f1_score(y_test,y_pred1)\n",
    "print(\"testing acc \",scores1)\n",
    "print(\"testing f1 \",f11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if model_mlp.solver == 'sgd' or model_mlp.solver=='adam':\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(model_mlp.loss_curve_)\n",
    "    plt.xlabel(\"iters\")\n",
    "    plt.ylabel(model_mlp.loss)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_1 = confusion_matrix(y_test, y_pred1, labels=[0,1]).ravel()\n",
    "print(cm_1)\n",
    "# recall = cm_1[0][0]/(cm_1[0][0]+cm_1[0][1])\n",
    "recall = cm_1[3]/(cm_1[3]+cm_1[2])\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "y_val = y_train[:1000]\n",
    "x_train = x_train[1000:]\n",
    "y_train = y_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(32,)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(64, activation='relu',input_shape=(32,)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.4), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "from keras import backend as K\n",
    "def f1(y_true, y_pred):\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    SVMSMOTEpredicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    f1 = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer= \"rmsprop\", metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer= \"rmsprop\", metrics=[f1,'accuracy','AUC'])\n",
    "history = model.fit(x_train, y_train, epochs=200,shuffle=True, validation_data=(x_val,y_val),batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(x_test)\n",
    "y_pred2[y_pred2>0.5]=1\n",
    "y_pred2[y_pred2<0.5]=0\n",
    "scores2 = metrics.accuracy_score(y_test,y_pred2)\n",
    "print(\"testing acc \",scores2)\n",
    "model.evaluate(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_2 = confusion_matrix(y_test, y_pred2, labels=[0,1])\n",
    "print(cm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# val_1 = {\"epochs\": epochs, \"val_loss\": val_loss_values}\n",
    "# val_pd_1 = pd.DataFrame(val_1)\n",
    "# val_pd_1[\"val_loss\"].min() # find the epochs with the minimum validation loss\n",
    "# print(val_pd_1[val_pd_1[\"val_loss\"] == val_pd_1[\"val_loss\"].min()]) \n",
    "\n",
    "#Visulization of validation loss\n",
    "plt.plot(epochs, loss_values,color = 'lightseagreen', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values,color = 'lightcoral', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation accuracy\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "acc = history.history['f1']\n",
    "val_acc = history.history['val_f1']\n",
    "\n",
    "# val_2 = {\"epochs\": epochs, \"val_acc\": val_acc}\n",
    "# val_pd_2 = pd.DataFrame(val_2)\n",
    "# val_pd_2[\"val_acc\"].max() # find the epochs with the maximum accuracy\n",
    "# print(val_pd_2[val_pd_2[\"val_acc\"] == val_pd_2[\"val_acc\"].max()]) \n",
    "\n",
    "#Visulization of accuracy\n",
    "plt.plot(epochs, acc,color = 'lightseagreen', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc,color = 'lightcoral', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a53d216d604efa81e45002259db34e1fa64cdea6c0d89e56d5d0492204834cb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
